{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification d'images - Modèle VGG16 \n",
    "\n",
    "# Introduction\n",
    "\n",
    "Nous avons dans une première partie essayer de classifier nos images de chiens en fonction de leurs races en utilisant les méthodes conventionnels de traitement d'images et de clustering non supervisé. \n",
    "\n",
    "Nous allons maintenant utiliser des méthodes de deep learning. Nous n'allons pas entrainer tous le réseaux de neurones mais allons utiliser un réseaux de neurones pré-entrainer: le VGG-16. Ce réseau de neurones a été entrainé sur différents types d'images via des couches de convolutions et des couches de classifications classiques.\n",
    "\n",
    "# Prétraitement des données\n",
    "\n",
    "Nous allons tout d'abord simplement entrainer nos données avec 10 classes de chiens ce qui nous donnera une assez bonne vue du fonctionnement de nos algorihtmes. Nous allons ensuite testés nos résultats avec plus de classes pour observer les résultats finaux.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zakis\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\zakis\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, Model\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "directory = r'C:\\Users\\zakis\\Documents\\OpenClassroom\\Projet 7\\Images'\n",
    "n_class = 10#♣len(os.listdir(directory))\n",
    "\n",
    "nb_photos = 1000\n",
    "Y = []\n",
    "X = []\n",
    "for i in range(0, n_class):#len(os.listdir(directory))):\n",
    "    folder = directory + '\\\\' + os.listdir(directory)[i]\n",
    "    nb = min(nb_photos, len(os.listdir(folder)))\n",
    "    for j in range(0, nb):\n",
    "        file = os.listdir(folder)[j]\n",
    "        img = folder + '\\\\' + file\n",
    "        img = load_img(img, target_size=(128, 128))  # Charger l'image\n",
    "        img = img_to_array(img)  # Convertir en tableau numpy\n",
    "        img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))  # Créer la collection d'images (un seul échantillon)\n",
    "        img = preprocess_input(img)  # Prétraiter l'image comme le veut VGG-16\n",
    "        y = np.zeros((n_class,1))\n",
    "        y[i] = 1\n",
    "        if len(X) == 0:\n",
    "            X = img\n",
    "        else:\n",
    "            X = np.concatenate([X, img])\n",
    "        Y.append(y)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y).reshape((len(Y),n_class))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# VGG-16 - customisation basique\n",
    "\n",
    "Nous allons tout d'abord essayer d'entrainer notre réseaux de neurones en enlevant les couches de classifications relatives du réseaux vgg16 et en les remplaçant par une classification que nous allons entrainer à dix classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1535 samples, validate on 384 samples\n",
      "Epoch 1/100\n",
      "1535/1535 [==============================] - 502s 327ms/step - loss: 12.0346 - acc: 0.1831 - val_loss: 9.7180 - val_acc: 0.3359\n",
      "Epoch 2/100\n",
      "1535/1535 [==============================] - 603s 393ms/step - loss: 8.9564 - acc: 0.3915 - val_loss: 8.1345 - val_acc: 0.4453\n",
      "Epoch 3/100\n",
      "1535/1535 [==============================] - 583s 380ms/step - loss: 7.6783 - acc: 0.4827 - val_loss: 7.8496 - val_acc: 0.4453\n",
      "Epoch 4/100\n",
      "1535/1535 [==============================] - 569s 371ms/step - loss: 6.6858 - acc: 0.5394 - val_loss: 6.7237 - val_acc: 0.5130\n",
      "Epoch 5/100\n",
      "1535/1535 [==============================] - 626s 408ms/step - loss: 5.4948 - acc: 0.6261 - val_loss: 5.9555 - val_acc: 0.5260\n",
      "Epoch 6/100\n",
      "1535/1535 [==============================] - 469s 305ms/step - loss: 4.2343 - acc: 0.6827 - val_loss: 5.3523 - val_acc: 0.5807\n",
      "Epoch 7/100\n",
      "1535/1535 [==============================] - 562s 366ms/step - loss: 3.3922 - acc: 0.7531 - val_loss: 4.8584 - val_acc: 0.6198\n",
      "Epoch 8/100\n",
      "1535/1535 [==============================] - 547s 357ms/step - loss: 2.9269 - acc: 0.7857 - val_loss: 4.5580 - val_acc: 0.6432\n",
      "Epoch 9/100\n",
      "1535/1535 [==============================] - 598s 389ms/step - loss: 2.5721 - acc: 0.8313 - val_loss: 4.2876 - val_acc: 0.6432\n",
      "Epoch 10/100\n",
      "1535/1535 [==============================] - 523s 341ms/step - loss: 2.4357 - acc: 0.8345 - val_loss: 4.0572 - val_acc: 0.6719\n",
      "Epoch 11/100\n",
      "1535/1535 [==============================] - 475s 310ms/step - loss: 2.2870 - acc: 0.8521 - val_loss: 3.9366 - val_acc: 0.6615\n",
      "Epoch 12/100\n",
      "1535/1535 [==============================] - 409s 267ms/step - loss: 2.2298 - acc: 0.8593 - val_loss: 4.0249 - val_acc: 0.6562\n",
      "Epoch 13/100\n",
      "1535/1535 [==============================] - 406s 264ms/step - loss: 2.2063 - acc: 0.8619 - val_loss: 4.0376 - val_acc: 0.6667\n",
      "Epoch 14/100\n",
      "1535/1535 [==============================] - 403s 262ms/step - loss: 2.2091 - acc: 0.8619 - val_loss: 4.0250 - val_acc: 0.6693\n",
      "Epoch 15/100\n",
      "1535/1535 [==============================] - 463s 302ms/step - loss: 2.1888 - acc: 0.8625 - val_loss: 4.2020 - val_acc: 0.6615\n",
      "Epoch 16/100\n",
      "1535/1535 [==============================] - 463s 302ms/step - loss: 2.1414 - acc: 0.8612 - val_loss: 3.9707 - val_acc: 0.6771\n"
     ]
    }
   ],
   "source": [
    "side_length = 128\n",
    "vgg16_base = VGG16(include_top=False, \n",
    "                   weights='imagenet', \n",
    "                   input_shape=(side_length, side_length, 3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg16_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "vgg16_base.trainable = False\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5),\n",
    "    ModelCheckpoint('vgg16_simple.h5', save_best_only=True),\n",
    "]\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train,  \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    batch_size=32, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ici utilisé le modèle vgg16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associé avec un classifieur qui nous donne le modèle suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2048)              16779264  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 31,514,442\n",
      "Trainable params: 16,799,754\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a vu que nos résultats très améliorés par rapport aux résultats avec les méthodes classiques de classification. On a un score (accuracy) de 67%. Nous allons essayer d'améliorer nos résultats en rajoutant une couche de classification à celle déjà existante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1535 samples, validate on 384 samples\n",
      "Epoch 1/100\n",
      "1535/1535 [==============================] - 501s 327ms/step - loss: 9.9916 - acc: 0.3440 - val_loss: 7.1879 - val_acc: 0.5234\n",
      "Epoch 2/100\n",
      "1535/1535 [==============================] - 412s 269ms/step - loss: 6.5368 - acc: 0.5655 - val_loss: 5.3872 - val_acc: 0.6354\n",
      "Epoch 3/100\n",
      "1535/1535 [==============================] - 401s 261ms/step - loss: 5.4235 - acc: 0.6430 - val_loss: 5.6336 - val_acc: 0.6328\n",
      "Epoch 4/100\n",
      "1535/1535 [==============================] - 399s 260ms/step - loss: 5.1668 - acc: 0.6573 - val_loss: 5.5609 - val_acc: 0.6302\n",
      "Epoch 5/100\n",
      "1535/1535 [==============================] - 396s 258ms/step - loss: 4.8121 - acc: 0.6847 - val_loss: 4.3949 - val_acc: 0.6979\n",
      "Epoch 6/100\n",
      "1535/1535 [==============================] - 394s 257ms/step - loss: 4.1443 - acc: 0.7238 - val_loss: 3.7883 - val_acc: 0.7448\n",
      "Epoch 7/100\n",
      "1535/1535 [==============================] - 395s 257ms/step - loss: 3.4344 - acc: 0.7687 - val_loss: 4.2564 - val_acc: 0.7135\n",
      "Epoch 8/100\n",
      "1535/1535 [==============================] - 396s 258ms/step - loss: 3.5415 - acc: 0.7674 - val_loss: 4.2293 - val_acc: 0.7083\n",
      "Epoch 9/100\n",
      "1535/1535 [==============================] - 396s 258ms/step - loss: 3.2898 - acc: 0.7850 - val_loss: 3.6575 - val_acc: 0.7630\n",
      "Epoch 10/100\n",
      "1535/1535 [==============================] - 396s 258ms/step - loss: 3.0164 - acc: 0.8046 - val_loss: 3.9857 - val_acc: 0.7370\n",
      "Epoch 11/100\n",
      "1535/1535 [==============================] - 395s 257ms/step - loss: 2.9490 - acc: 0.8065 - val_loss: 3.6930 - val_acc: 0.7552\n",
      "Epoch 12/100\n",
      "1535/1535 [==============================] - 395s 257ms/step - loss: 2.9159 - acc: 0.8098 - val_loss: 3.9793 - val_acc: 0.7318\n",
      "Epoch 13/100\n",
      "1535/1535 [==============================] - 396s 258ms/step - loss: 2.8570 - acc: 0.8104 - val_loss: 3.7350 - val_acc: 0.7474\n",
      "Epoch 14/100\n",
      "1535/1535 [==============================] - 396s 258ms/step - loss: 2.5620 - acc: 0.8287 - val_loss: 3.6654 - val_acc: 0.7630\n"
     ]
    }
   ],
   "source": [
    "side_length = 128\n",
    "vgg16_base = VGG16(include_top=False, \n",
    "                   weights='imagenet', \n",
    "                   input_shape=(side_length, side_length, 3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg16_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "vgg16_base.trainable = False\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5),\n",
    "    ModelCheckpoint('vgg16_simple.h5', save_best_only=True),\n",
    "]\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train,  \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    batch_size=32, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons que les résultats sont encore améliorés et permettent d'atteindre un score de 76% de prédictions correctes. \n",
    "\n",
    "Nous avons donc bien classifier nos 10 classes de chiens. Nous allons encore essayer d'améliorer nos résultats en entrainant cette fois les dernières couches de convolutions. Celles-ci correspondent aux détails de l'image assez larges comme la forme des oreilles ou des yeux. En entrainant ces couches on espère avoir des résultats encore meilleurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1535 samples, validate on 384 samples\n",
      "Epoch 1/100\n",
      "1535/1535 [==============================] - 542s 353ms/step - loss: 12.2945 - acc: 0.2215 - val_loss: 10.0467 - val_acc: 0.3620\n",
      "Epoch 2/100\n",
      "1535/1535 [==============================] - 527s 344ms/step - loss: 10.0967 - acc: 0.3388 - val_loss: 9.5396 - val_acc: 0.3542\n",
      "Epoch 3/100\n",
      "1535/1535 [==============================] - 532s 346ms/step - loss: 4.9607 - acc: 0.2358 - val_loss: 2.0800 - val_acc: 0.2630\n",
      "Epoch 4/100\n",
      "1535/1535 [==============================] - 518s 338ms/step - loss: 2.0658 - acc: 0.2801 - val_loss: 1.8448 - val_acc: 0.3568\n",
      "Epoch 5/100\n",
      "1535/1535 [==============================] - 566s 369ms/step - loss: 1.8539 - acc: 0.3537 - val_loss: 1.7526 - val_acc: 0.3776\n",
      "Epoch 6/100\n",
      "1535/1535 [==============================] - 502s 327ms/step - loss: 1.7326 - acc: 0.3987 - val_loss: 1.8177 - val_acc: 0.4036\n",
      "Epoch 7/100\n",
      "1535/1535 [==============================] - 492s 320ms/step - loss: 1.5792 - acc: 0.4482 - val_loss: 1.6665 - val_acc: 0.4297\n",
      "Epoch 8/100\n",
      "1535/1535 [==============================] - 520s 339ms/step - loss: 1.4699 - acc: 0.4814 - val_loss: 1.7009 - val_acc: 0.4349\n",
      "Epoch 9/100\n",
      "1535/1535 [==============================] - 524s 341ms/step - loss: 1.3298 - acc: 0.5270 - val_loss: 1.5714 - val_acc: 0.4818\n",
      "Epoch 10/100\n",
      "1535/1535 [==============================] - 520s 338ms/step - loss: 1.1822 - acc: 0.5726 - val_loss: 1.5064 - val_acc: 0.5234\n",
      "Epoch 11/100\n",
      "1535/1535 [==============================] - 518s 338ms/step - loss: 0.9857 - acc: 0.6365 - val_loss: 1.6292 - val_acc: 0.4688\n",
      "Epoch 12/100\n",
      "1535/1535 [==============================] - 491s 320ms/step - loss: 0.8590 - acc: 0.6984 - val_loss: 1.6224 - val_acc: 0.5703\n",
      "Epoch 13/100\n",
      "1535/1535 [==============================] - 482s 314ms/step - loss: 0.6310 - acc: 0.7739 - val_loss: 1.3998 - val_acc: 0.5938\n",
      "Epoch 14/100\n",
      "1535/1535 [==============================] - 504s 328ms/step - loss: 0.4423 - acc: 0.8358 - val_loss: 1.6341 - val_acc: 0.6042\n",
      "Epoch 15/100\n",
      "1535/1535 [==============================] - 530s 345ms/step - loss: 0.3410 - acc: 0.8827 - val_loss: 1.6276 - val_acc: 0.5990\n",
      "Epoch 16/100\n",
      "1535/1535 [==============================] - 530s 345ms/step - loss: 0.2393 - acc: 0.9212 - val_loss: 1.6075 - val_acc: 0.6068\n",
      "Epoch 17/100\n",
      "1535/1535 [==============================] - 494s 322ms/step - loss: 0.1600 - acc: 0.9485 - val_loss: 1.7392 - val_acc: 0.5911\n",
      "Epoch 18/100\n",
      "1535/1535 [==============================] - 525s 342ms/step - loss: 0.1217 - acc: 0.9590 - val_loss: 1.8913 - val_acc: 0.5990\n"
     ]
    }
   ],
   "source": [
    "side_length = 128\n",
    "vgg16_base = VGG16(include_top=False, \n",
    "                   weights='imagenet', \n",
    "                   input_shape=(side_length, side_length, 3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg16_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "vgg16_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in vgg16_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    layer.trainable = set_trainable\n",
    "    \n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5),\n",
    "    ModelCheckpoint('vgg16_simple.h5', save_best_only=True),\n",
    "]\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train,  \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    batch_size=32, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On avons entrainer notre algorithme avec toutes les couches de convoutions pré entrainer du reseaux vgg16 et avons entrainer les dernières couches.\n",
    "\n",
    "Chaque couche de convolutions correspond aux détails de plus en plus complexes que l'on a pu trouver dans les images. Nous allons retirer la dernière couche de convolution du réseaux vg16 et entrainer l'avant dernière couche de convolution de tels sortes que l'on enlève un peu de la complexité des détails trouver par le réseaux vgg16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "side_length = 128\n",
    "vgg16_base = VGG16(include_top=False, \n",
    "                   weights='imagenet', \n",
    "                   input_shape=(side_length, side_length, 3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg16_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "vgg16_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in vgg16_base.layers:\n",
    "    if layer.name == 'block4_conv1':\n",
    "        set_trainable = True\n",
    "    layer.trainable = set_trainable\n",
    "    \n",
    "for layer in vgg16_base.layers[-4:-1]:\n",
    "    vgg16_base.layers.remove(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 7,635,264\n",
      "Trainable params: 5,899,776\n",
      "Non-trainable params: 1,735,488\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1535 samples, validate on 384 samples\n",
      "Epoch 1/100\n",
      "1535/1535 [==============================] - 703s 458ms/step - loss: 6.9523 - acc: 0.1590 - val_loss: 2.2292 - val_acc: 0.2422\n",
      "Epoch 2/100\n",
      "1535/1535 [==============================] - 675s 440ms/step - loss: 2.1438 - acc: 0.2423 - val_loss: 1.8432 - val_acc: 0.3255\n",
      "Epoch 3/100\n",
      "1535/1535 [==============================] - 626s 408ms/step - loss: 1.7558 - acc: 0.3876 - val_loss: 1.5897 - val_acc: 0.4505\n",
      "Epoch 4/100\n",
      "1535/1535 [==============================] - 614s 400ms/step - loss: 1.3498 - acc: 0.5309 - val_loss: 1.4080 - val_acc: 0.5026\n",
      "Epoch 5/100\n",
      "1535/1535 [==============================] - 646s 421ms/step - loss: 1.0562 - acc: 0.6313 - val_loss: 1.4056 - val_acc: 0.5443\n",
      "Epoch 6/100\n",
      "1535/1535 [==============================] - 683s 445ms/step - loss: 0.7582 - acc: 0.7342 - val_loss: 1.2746 - val_acc: 0.5677\n",
      "Epoch 7/100\n",
      "1535/1535 [==============================] - 637s 415ms/step - loss: 0.5274 - acc: 0.8235 - val_loss: 1.3101 - val_acc: 0.5755\n",
      "Epoch 8/100\n",
      "1535/1535 [==============================] - 624s 407ms/step - loss: 0.4062 - acc: 0.8638 - val_loss: 1.3587 - val_acc: 0.6016\n",
      "Epoch 9/100\n",
      "1535/1535 [==============================] - 625s 407ms/step - loss: 0.3062 - acc: 0.8990 - val_loss: 1.3415 - val_acc: 0.6094\n",
      "Epoch 10/100\n",
      "1535/1535 [==============================] - 615s 401ms/step - loss: 0.1854 - acc: 0.9401 - val_loss: 1.5644 - val_acc: 0.6042\n",
      "Epoch 11/100\n",
      "1535/1535 [==============================] - 610s 398ms/step - loss: 0.1428 - acc: 0.9590 - val_loss: 1.5324 - val_acc: 0.6172\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=1e-4), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5),\n",
    "    ModelCheckpoint('vgg16_simple.h5', save_best_only=True),\n",
    "]\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train,  \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    batch_size=32, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "side_length = 128\n",
    "vgg16_base = VGG16(include_top=False, \n",
    "                   weights='imagenet', \n",
    "                   input_shape=(side_length, side_length, 3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg16_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "vgg16_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in vgg16_base.layers:\n",
    "    if layer.name == 'block3_conv1':\n",
    "        set_trainable = True\n",
    "    layer.trainable = set_trainable\n",
    "    \n",
    "for layer in vgg16_base.layers[-4:-1]:\n",
    "    vgg16_base.layers.remove(layer)\n",
    "for layer in vgg16_base.layers[-5:-2]:\n",
    "    vgg16_base.layers.remove(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,735,488\n",
      "Trainable params: 1,475,328\n",
      "Non-trainable params: 260,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1535 samples, validate on 384 samples\n",
      "Epoch 1/100\n",
      "1535/1535 [==============================] - 813s 530ms/step - loss: 9.1054 - acc: 0.2821 - val_loss: 3.3443 - val_acc: 0.2266\n",
      "Epoch 2/100\n",
      "1535/1535 [==============================] - 717s 467ms/step - loss: 2.8527 - acc: 0.1818 - val_loss: 2.0395 - val_acc: 0.2969\n",
      "Epoch 3/100\n",
      "1535/1535 [==============================] - 701s 457ms/step - loss: 1.9427 - acc: 0.3316 - val_loss: 1.7619 - val_acc: 0.3724\n",
      "Epoch 4/100\n",
      "1535/1535 [==============================] - 701s 457ms/step - loss: 1.4965 - acc: 0.4827 - val_loss: 1.6808 - val_acc: 0.4141\n",
      "Epoch 5/100\n",
      "1535/1535 [==============================] - 808s 527ms/step - loss: 1.2381 - acc: 0.5772 - val_loss: 1.5863 - val_acc: 0.4792\n",
      "Epoch 6/100\n",
      "1535/1535 [==============================] - 818s 533ms/step - loss: 0.9917 - acc: 0.6671 - val_loss: 1.5697 - val_acc: 0.4974\n",
      "Epoch 7/100\n",
      "1535/1535 [==============================] - 722s 470ms/step - loss: 0.7733 - acc: 0.7433 - val_loss: 1.5116 - val_acc: 0.5234\n",
      "Epoch 8/100\n",
      "1535/1535 [==============================] - 704s 458ms/step - loss: 0.5344 - acc: 0.8326 - val_loss: 1.5227 - val_acc: 0.5234\n",
      "Epoch 9/100\n",
      "1535/1535 [==============================] - 750s 489ms/step - loss: 0.4435 - acc: 0.8476 - val_loss: 1.6381 - val_acc: 0.5286\n",
      "Epoch 10/100\n",
      "1535/1535 [==============================] - 838s 546ms/step - loss: 0.3376 - acc: 0.8873 - val_loss: 1.6177 - val_acc: 0.5417\n",
      "Epoch 11/100\n",
      "1535/1535 [==============================] - 803s 523ms/step - loss: 0.2389 - acc: 0.9316 - val_loss: 1.8220 - val_acc: 0.5365\n",
      "Epoch 12/100\n",
      "1535/1535 [==============================] - 729s 475ms/step - loss: 0.1855 - acc: 0.9485 - val_loss: 1.6874 - val_acc: 0.5599\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=1e-4), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5),\n",
    "    ModelCheckpoint('vgg16_simple.h5', save_best_only=True),\n",
    "]\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train,  \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    batch_size=32, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "side_length = 128\n",
    "vgg16_base = VGG16(include_top=False, \n",
    "                   weights='imagenet', \n",
    "                   input_shape=(side_length, side_length, 3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg16_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "vgg16_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in vgg16_base.layers:\n",
    "    if layer.name == 'block2_conv1':\n",
    "        set_trainable = True\n",
    "    layer.trainable = set_trainable\n",
    "    \n",
    "for layer in vgg16_base.layers[-4:-1]:\n",
    "    vgg16_base.layers.remove(layer)\n",
    "for layer in vgg16_base.layers[-5:-2]:\n",
    "    vgg16_base.layers.remove(layer)\n",
    "for layer in vgg16_base.layers[-6:-3]:\n",
    "    vgg16_base.layers.remove(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 260,160\n",
      "Trainable params: 221,440\n",
      "Non-trainable params: 38,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1535 samples, validate on 384 samples\n",
      "Epoch 1/100\n",
      "1535/1535 [==============================] - 853s 555ms/step - loss: 11.6149 - acc: 0.2515 - val_loss: 10.2276 - val_acc: 0.3385\n",
      "Epoch 2/100\n",
      "1535/1535 [==============================] - 799s 520ms/step - loss: 9.0997 - acc: 0.4130 - val_loss: 7.9828 - val_acc: 0.4870\n",
      "Epoch 3/100\n",
      "1535/1535 [==============================] - 720s 469ms/step - loss: 8.0098 - acc: 0.4801 - val_loss: 7.1364 - val_acc: 0.5365\n",
      "Epoch 4/100\n",
      "1535/1535 [==============================] - 714s 465ms/step - loss: 7.1226 - acc: 0.5381 - val_loss: 6.3836 - val_acc: 0.5755\n",
      "Epoch 5/100\n",
      "1535/1535 [==============================] - 715s 466ms/step - loss: 6.5070 - acc: 0.5818 - val_loss: 6.5134 - val_acc: 0.5833\n",
      "Epoch 6/100\n",
      "1535/1535 [==============================] - 712s 464ms/step - loss: 6.2798 - acc: 0.5935 - val_loss: 6.2191 - val_acc: 0.5911\n",
      "Epoch 7/100\n",
      "1535/1535 [==============================] - 710s 463ms/step - loss: 6.1604 - acc: 0.6026 - val_loss: 6.8108 - val_acc: 0.5495\n",
      "Epoch 8/100\n",
      "1535/1535 [==============================] - 709s 462ms/step - loss: 6.0521 - acc: 0.6104 - val_loss: 6.3721 - val_acc: 0.5781\n",
      "Epoch 9/100\n",
      "1535/1535 [==============================] - 711s 463ms/step - loss: 5.6576 - acc: 0.6384 - val_loss: 6.5162 - val_acc: 0.5859\n",
      "Epoch 10/100\n",
      "1535/1535 [==============================] - 710s 462ms/step - loss: 5.8263 - acc: 0.6267 - val_loss: 6.6653 - val_acc: 0.5755\n",
      "Epoch 11/100\n",
      "1535/1535 [==============================] - 711s 463ms/step - loss: 5.3345 - acc: 0.6606 - val_loss: 6.3001 - val_acc: 0.5833\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=1e-4), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5),\n",
    "    ModelCheckpoint('vgg16_simple.h5', save_best_only=True),\n",
    "]\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train,  \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    batch_size=32, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est arrivé à une valeur du montant accuracy à 58% ce qui nous indique que le modèle le plus performant est celui qui prend 3 couches de convolutions: 2 couches préentrainées et une couche entrainé via nos données. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant tester avec plus de races de chiens les résultats. Malheureusement, on ne va aller au delà de 25 races pour des raisons de temps de calculs et de mémoires trop faibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "directory = r'C:\\Users\\zakis\\Documents\\OpenClassroom\\Projet 7\\Images'\n",
    "n_class = 25#♣len(os.listdir(directory))\n",
    "\n",
    "nb_photos = 1000\n",
    "Y = []\n",
    "X = []\n",
    "for i in range(0, n_class):#len(os.listdir(directory))):\n",
    "    folder = directory + '\\\\' + os.listdir(directory)[i]\n",
    "    nb = min(nb_photos, len(os.listdir(folder)))\n",
    "    for j in range(0, nb):\n",
    "        file = os.listdir(folder)[j]\n",
    "        img = folder + '\\\\' + file\n",
    "        img = load_img(img, target_size=(128, 128))  # Charger l'image\n",
    "        img = img_to_array(img)  # Convertir en tableau numpy\n",
    "        img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))  # Créer la collection d'images (un seul échantillon)\n",
    "        img = preprocess_input(img)  # Prétraiter l'image comme le veut VGG-16\n",
    "        y = np.zeros((n_class,1))\n",
    "        y[i] = 1\n",
    "        if len(X) == 0:\n",
    "            X = img\n",
    "        else:\n",
    "            X = np.concatenate([X, img])\n",
    "        Y.append(y)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y).reshape((len(Y),n_class))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3629 samples, validate on 908 samples\n",
      "Epoch 1/100\n",
      "3629/3629 [==============================] - 1355s 373ms/step - loss: 5.9958 - acc: 0.0507 - val_loss: 3.2134 - val_acc: 0.0562\n",
      "Epoch 2/100\n",
      "3629/3629 [==============================] - 1367s 377ms/step - loss: 3.2162 - acc: 0.0526 - val_loss: 3.2046 - val_acc: 0.0452\n",
      "Epoch 3/100\n",
      "3629/3629 [==============================] - 1259s 347ms/step - loss: 3.1951 - acc: 0.0562 - val_loss: 3.1654 - val_acc: 0.0859\n",
      "Epoch 4/100\n",
      "3629/3629 [==============================] - 1369s 377ms/step - loss: 3.1352 - acc: 0.0656 - val_loss: 3.1234 - val_acc: 0.0870\n",
      "Epoch 5/100\n",
      "3629/3629 [==============================] - 1360s 375ms/step - loss: 3.0562 - acc: 0.0818 - val_loss: 3.0195 - val_acc: 0.1256\n",
      "Epoch 6/100\n",
      "3629/3629 [==============================] - 1368s 377ms/step - loss: 2.9286 - acc: 0.1309 - val_loss: 2.8936 - val_acc: 0.1300\n",
      "Epoch 7/100\n",
      "3629/3629 [==============================] - 1271s 350ms/step - loss: 2.7779 - acc: 0.1667 - val_loss: 2.8246 - val_acc: 0.1751\n",
      "Epoch 8/100\n",
      "3629/3629 [==============================] - 1269s 350ms/step - loss: 2.6160 - acc: 0.2144 - val_loss: 2.8288 - val_acc: 0.1630\n",
      "Epoch 9/100\n",
      "3629/3629 [==============================] - 1300s 358ms/step - loss: 2.4442 - acc: 0.2643 - val_loss: 2.6486 - val_acc: 0.2214\n",
      "Epoch 10/100\n",
      "3629/3629 [==============================] - 1268s 349ms/step - loss: 2.1967 - acc: 0.3359 - val_loss: 2.4370 - val_acc: 0.2952\n",
      "Epoch 11/100\n",
      "3629/3629 [==============================] - 1291s 356ms/step - loss: 1.8453 - acc: 0.4274 - val_loss: 2.4208 - val_acc: 0.3161\n",
      "Epoch 12/100\n",
      "3629/3629 [==============================] - 1348s 371ms/step - loss: 1.4805 - acc: 0.5324 - val_loss: 2.2980 - val_acc: 0.3359\n",
      "Epoch 13/100\n",
      "3629/3629 [==============================] - 1143s 315ms/step - loss: 1.0782 - acc: 0.6630 - val_loss: 2.3857 - val_acc: 0.3667\n",
      "Epoch 14/100\n",
      "3629/3629 [==============================] - 1254s 346ms/step - loss: 0.7340 - acc: 0.7732 - val_loss: 2.3811 - val_acc: 0.3965\n",
      "Epoch 15/100\n",
      "3629/3629 [==============================] - 1275s 351ms/step - loss: 0.4055 - acc: 0.8688 - val_loss: 2.5115 - val_acc: 0.4163\n",
      "Epoch 16/100\n",
      "3629/3629 [==============================] - 1267s 349ms/step - loss: 0.2708 - acc: 0.9157 - val_loss: 2.6049 - val_acc: 0.3954\n",
      "Epoch 17/100\n",
      "3629/3629 [==============================] - 1230s 339ms/step - loss: 0.1803 - acc: 0.9452 - val_loss: 2.7805 - val_acc: 0.4383\n"
     ]
    }
   ],
   "source": [
    "side_length = 128\n",
    "vgg16_base = VGG16(include_top=False, \n",
    "                   weights='imagenet', \n",
    "                   input_shape=(side_length, side_length, 3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg16_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(25, activation='softmax'))\n",
    "\n",
    "vgg16_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in vgg16_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    layer.trainable = set_trainable\n",
    "    \n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-4), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5),\n",
    "    ModelCheckpoint('vgg16_simple.h5', save_best_only=True),\n",
    "]\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train,  \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    batch_size=32, epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit toujours un très bon résultat:45% de réussites pour 25 classes nous n'allons pas testé d'autres algorihtmes et allons nous contenter de ces résultats pour le fine tunning du réseaux vgg16. Ce réseau nous permet donc de trouver un résultat bien meilleure qu'avec des méthodes classiques. Cependant on voit que l'on a un résultat qui reste insufisant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Nous avons tester le fine tunning du réseaux vgg16. Nous avons tout d'abord tester une extraction de features entrainer avec nos images et nous avons observer des résultats meilleures qu'avec les méthodes traditionnels d'extractions de features.\n",
    "\n",
    "Nous avons cependant des résultats qui ne nous parraissent pas suffisants pour classifier des chiens et allons essayer dans une seconde partie de tunner un autre réseau de neurones le resnet50."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
